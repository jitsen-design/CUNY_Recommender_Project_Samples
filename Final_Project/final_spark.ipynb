{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Evaluating Ideal Number of Weights Using Spark\n",
    "\n",
    "For this exercise, we'll use Spark on an AWS EC2 machine. \n",
    "\n",
    "### AWS Machine Specs \n",
    "The machine is a T2 Extra Large machine with 8 cores and 32GB of RAM.\n",
    "\n",
    "### Spark Setup\n",
    "We'll use PySpark with two partitions. We are running four cores, and Spark with use these to run parallel jobs\n",
    "\n",
    "### Objective\n",
    "The objective of this exercise is to look evaluate the optimal number of user and item weights using Spark. We are using distributed systems because the computational power requied for these large sparse matrices is very high.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* Book Review Data (From a previous exercise)\n",
    "* Jester Dataset (Used in this exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Libraries for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, os, sys, zipfile, io\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "#import pyspark\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import json\n",
    "import surprise\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from pyspark.sql import functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Spark\n",
    "\n",
    "### Open Secrets File to Get Path to Spark Master \n",
    "\n",
    "(I don't want my internal IP in the public domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/secrets.json') as f:\n",
    "    pyspark_config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session with Link to Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.master(pyspark_config['master']).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Default Parallelism (since we have eight cores)\n",
    "\n",
    "We can see that spark has automatically detected that we can run eight parallel operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Import into Spark RDD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings_books = spark.read.csv('data/ratings_books.csv',\n",
    "                        sep=',',\n",
    "                        inferSchema=True,\n",
    "                        header=True)\n",
    "                         \n",
    "                         \n",
    "\n",
    "for i in ['user_id', \n",
    "         'book_id']:\n",
    "          ratings_books = ratings_books.withColumn(i, ratings_books[i].cast('int'))\n",
    "    \n",
    "ratings_books = ratings_books.withColumn(\"rating\", ratings_books[\"rating\"].cast('float'))\n",
    "\n",
    "ratings_books = ratings_books[['user_id',\n",
    "                             'book_id',\n",
    "                             'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Train Split for Book Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_books, test_books = ratings_books.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark ALS Instance and get Cross Val Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ALS instance\n",
    "als_book = ALS(userCol='user_id',\n",
    "              itemCol='book_id',\n",
    "              ratingCol='rating',\n",
    "              nonnegative=True,\n",
    "              seed=42)\n",
    "\n",
    "# The parameter grid to search\n",
    "\n",
    "als_book_paramgrid = (ParamGridBuilder()\n",
    "                 .addGrid(als_book.rank, [10, 20])\n",
    "                 .addGrid(als_book.maxIter, [10])\n",
    "                 .addGrid(als_book.regParam, [0.1, 0.5, 1.0])\n",
    "                 .addGrid(als_book.alpha, [0.1, 0.5, 1.0])\n",
    "                 .build())\n",
    "\n",
    "# The evaluation function for determining the best model\n",
    "rmse_eval_book = RegressionEvaluator(labelCol='rating',\n",
    "                                predictionCol='prediction', \n",
    "                                metricName='rmse')\n",
    "\n",
    "# The cross validation instance\n",
    "cv_book = CrossValidator(estimator=als_book,\n",
    "                    parallelism=8,\n",
    "                    collectSubModels=True,\n",
    "                    estimatorParamMaps=als_book_paramgrid,\n",
    "                    evaluator=rmse_eval_book,\n",
    "                    numFolds=3, \n",
    "                    seed=42)\n",
    "\n",
    "# Fit the models and find the best one!\n",
    "als_book_cv = cv_book.fit(train_books.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Number of Weights for Best Model (Book Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Item_Factors': 10, 'Rank': 10, 'User_Factors': 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_book_best = als_book_cv.bestModel\n",
    "dict(Rank=als_book_best.rank,\n",
    "     User_Factors=len(als_book_best.itemFactors.toPandas()['features'][0]),\n",
    "     Item_Factors=len(als_book_best.userFactors.toPandas()['features'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|1.2680396014053497|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the aggregate function can be created outside of the dataframe if desired\n",
    "rms_calc = func.sqrt(func.mean('diff_sq'))\n",
    "als_book_pred = als_book_best.transform(test_books)\n",
    "als_book_pred = als_book_pred.withColumn('diff_sq', (als_book_pred['rating'] - als_book_pred['prediction'])**2)\n",
    "als_book_pred.dropna().select(rms_calc.alias('rmse')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Jester Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_jokes = spark.read.csv('data/jester_ratings.csv',\n",
    "                        sep=',',\n",
    "                        inferSchema=True,\n",
    "                        header=True)\n",
    "                         \n",
    "                         \n",
    "\n",
    "for i in ['user', \n",
    "          'item']:\n",
    "          ratings_jokes = ratings_jokes.withColumn(i, ratings_jokes[i].cast('int'))\n",
    "    \n",
    "ratings_jokes = ratings_jokes.withColumn(\"rating\", ratings_jokes[\"rating\"].cast('float'))\n",
    "\n",
    "ratings_jokes = ratings_jokes[['user',\n",
    "                             'item',\n",
    "                             'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_jokes, test_jokes = ratings_jokes.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ALS instance\n",
    "als_jokes = ALS(userCol='user',\n",
    "              itemCol='item',\n",
    "              ratingCol='rating',\n",
    "              nonnegative=False,\n",
    "              seed=42)\n",
    "\n",
    "# The parameter grid to search\n",
    "\n",
    "als_jokes_paramgrid = (ParamGridBuilder()\n",
    "                 .addGrid(als_jokes.rank, [10, 20])\n",
    "                 .addGrid(als_jokes.maxIter, [10])\n",
    "                 .addGrid(als_jokes.regParam, [0.1, 0.5, 1.0])\n",
    "                 .addGrid(als_jokes.alpha, [0.1, 0.5, 1.0])\n",
    "                 .build())\n",
    "\n",
    "# The evaluation function for determining the best model\n",
    "rmse_eval_jokes = RegressionEvaluator(labelCol='rating',\n",
    "                                predictionCol='prediction', \n",
    "                                metricName='rmse')\n",
    "\n",
    "# The cross validation instance\n",
    "cv_jokes = CrossValidator(estimator=als_jokes,\n",
    "                    parallelism=8,\n",
    "                    collectSubModels=True,\n",
    "                    estimatorParamMaps=als_jokes_paramgrid,\n",
    "                    evaluator=rmse_eval_jokes,\n",
    "                    numFolds=3, \n",
    "                    seed=42)\n",
    "\n",
    "# Fit the models and find the best one!\n",
    "als_jokes_cv = cv_jokes.fit(train_jokes.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Number of Weights for Best Model (Jester Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Item_Factors': 10, 'Rank': 10, 'User_Factors': 10}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_jokes_best = als_jokes_cv.bestModel\n",
    "dict(Rank=als_jokes_best.rank,\n",
    "     User_Factors=len(als_jokes_best.itemFactors.toPandas()['features'][0]),\n",
    "     Item_Factors=len(als_jokes_best.userFactors.toPandas()['features'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|4.521681825280404|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the aggregate function can be created outside of the dataframe if desired\n",
    "rms_calc = func.sqrt(func.mean('diff_sq'))\n",
    "als_jokes_pred = als_jokes_best.transform(test_jokes)\n",
    "als_jokes_pred = als_jokes_pred.withColumn('diff_sq', (als_jokes_pred['rating'] - als_jokes_pred['prediction'])**2)\n",
    "als_jokes_pred.dropna().select(rms_calc.alias('rmse')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "The RMSE for the Jester data seems large, but only because we have a rating from -10 to +10. However, it is clear that for both datasets, 10 is a better number of weights than 20"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
